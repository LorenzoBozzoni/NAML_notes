So far we have considered:
\[
    \underline{y} = X\underline{w} \hspace{1cm} X \in \mathbb{R}^{n \times p} \text{ and } \begin{Bmatrix}
        \text{Least Squares}: \underline{\hat{w}}_LS\\
        \text{Ridge Regression}: \underline{\hat{w}}_{RR}\\
    \end{Bmatrix} \underline{w} \in \mathbb{R}^p \text{ dense vector, i.e. not many zeros} 
\]
We are going to see a method which obtain a vector $\underline{\hat{w}}$ with many zeros, as sparse as possible. We want to consider this model:
\[
    \underline{y} = X\underline{w} \hspace{1cm} X \in \mathbb{R}^{n \times p} \hspace{1cm} p > n 
\]
We have said that the system is undetermined since it has infinite solutions. Suppose to have 2 features and 1 sample. 
\[
    X = \begin{bmatrix}
        2 & 3\\
    \end{bmatrix}
    \hspace{1cm}
    y = \begin{bmatrix}
        1\\
    \end{bmatrix}
\]
And so:
\[
    1 = \begin{bmatrix}
        2 & 3
    \end{bmatrix}\begin{bmatrix}
        w_1\\
        w_2
    \end{bmatrix} \implies
    1 = 2w_1 + 3w_2 \hspace{0.2cm} \leftarrow \hspace{0.2cm} \text{line}    
\]
As mentioned in a previous lecture, we want to find the minimum length solution so we can plot the line found before and the circles that represents the l2-norm distance.
\begin{center}
    \includegraphics[scale=0.6]{../images/LassoRidgePlot.png}
\end{center}
On the right-hand side it is represented the equivalent plot but with the l1-norm. We can see that the solution on the right has one coordinate (are features!) that is equal to zero. This is true in general, i.e. we will obtain sparser solution using the l1-norm rather than the l2-norm.\\
With L1-norm it's still a convex optimization problem and
\[
    F(\underline{w}) = ||X\underline{w} - \underline{y}||_2^2 + \lambda ||\underline{w}||_1    
\] 
This model is implemented by \textbf{Lasso} (Least Absolute Shrinkage and Selection Operator) and achieve both the shortest distance solution and the selection of some features.

This is important because by reducing the number of features, we increase the interpretability of the model.\\

There is also the \textbf{Elastic Net} which combines both Lasso and Ridge: 
\[
    F(\underline{w}) = ||X\underline{w} - \underline{y}||_2^2 + \lambda_1||\underline{w}||_1 + \lambda_2||\underline{w}||_2^2  
\] 

\vspace{2cm}
Once again, we start considering:
\[
    \underline{y} \in \mathbb{R}^n \hspace{1cm} X \in \mathbb{R}^{n \times p} \hspace{0.2cm} \text{with \emph{p} lin. ind. cols } (\implies \sigma_i > 0, \hspace{0.1cm} i = 1, \dots, p)  
\]
Now we write the formulation of the weights vector $w$ we found for Ridge Regression. 
\[
    \begin{split}
    \underline{\hat{w}}_R &= V\underbrace{(\Sigma^\intercal \Sigma + \lambda I)^{-1} \Sigma^\intercal}_{\substack{\Sigma^\intercal(\Sigma \Sigma^\intercal + \lambda I)^{-1}}} U^\intercal \underline{y}\\
    &= \underbrace{V\Sigma^\intercal U^\intercal}_{X^\intercal} \underbrace{U(\Sigma \Sigma^\intercal + \lambda I)^{-1} U^\intercal \underline{y}}_{\underline{\alpha} \in \mathbb{R}^n}\\
    &\overset{\boxdot}{=} X^\intercal \underline{\alpha}\\
    &= \sum_{i=1}^n \alpha_i \underline{x}_i\\
    \end{split}     
\]
In the second passage we have added the matrices $U^\intercal U$ because its the identity matrix. We obtain a weighted sum of the column vectors of $X^\intercal$, where $X$ is:
\[
X = \begin{bmatrix}
    \horzbar & \underline{x_1^\intercal} & \horzbar\\
    \horzbar & \underline{x_2^\intercal} & \horzbar\\    
     & \vdots & \\
    \horzbar & \underline{x_n^\intercal} & \horzbar
\end{bmatrix}
\]
So, until now, we have discussed about linear models. A generic form would be:
\[
    \hat{y}_i = w_1x_{i1} + w_2x_{i2} \hspace{1cm} \text{if} \hspace{1cm} \underline{x}_i = \begin{bmatrix}
        x_{i1}\\
        x_{i2}
    \end{bmatrix}    
\]
Now a new model:
\[
    \hat{y}_i = w_1x_{i1} + w_2x_{i2} + w_3x_{i1}^2 + w_4x_{i2}^2 + w_5x_{i1}x_{i2}    
\]
So, the original feature vector $\underline{x}$ is transformed into a new feature vector by means of function called feature map $\phi(x)$:
\[
    \phi(x) = \begin{bmatrix}
        x_1\\
        x_2\\
        x_1^2\\
        x_2^2\\
        x_1x_2
    \end{bmatrix} \in \mathbb{R}^d \hspace{1cm} d > p \text{ typically}    
\]
And
\[
    \hat{y}_i = \phi(x_i)^\intercal \underline{w}    
\]
In general $d$ can be huge.


\section{Kernel Methods}
The aim of this methods is to avoid the necessity of computing huge vectors.
\[
    \Phi =  \begin{bmatrix}
        \horzbar & \phi(\underline{x}_1)^\intercal & \horzbar\\
        \horzbar & \phi(\underline{x}_2)^\intercal & \horzbar\\    
         & \vdots & \\
        \horzbar & \phi(\underline{x}_n)^\intercal & \horzbar
    \end{bmatrix}
    \in \mathbb{R}^{n \times d} \hspace{1cm} \underline{\hat{y}} = \Phi \underline{w}
\]
The objective is still the same: i.e. finding $\underline{w}$. We are now going to consider ridge regression in order to achieve that. 
Instead of $\underline{\hat{w}}_R = X^\intercal \underline{\alpha}$ we can now write:
\[
    \underline{\hat{w}}_R = \Phi^\intercal \underline{\alpha}
\]
Where
\[
    \underline{\alpha} = U(\Sigma \Sigma^\intercal + \lambda I)^{-1} U^\intercal \underline{y} = (XX^\intercal + \lambda I)^{-1} \underline{y}    
\]
Here is the proof:
\[
    (U\Sigma V^\intercal V\Sigma^\intercal U^\intercal + \lambda UU^\intercal)^{-1} \underline{y}    
\]
So, by similarity with the expression obtained in the previous page ($\underline{\hat{w}}_R \overset{\boxdot}{=} X^\intercal \underline{\alpha}$) we can write:
\[
    \underline{\alpha} = (\Phi\Phi^\intercal + \lambda I)^{-1} \underline{y}    
\]
The computation of $\underline{\alpha}$ is practically impossible since $\Phi$ is huge. How can we reduce the cost of that computation? Introducing the \textbf{Kernel Trick of Kernel Function}:
\[
    K(\underline{x}_i, \underline{x}_j) = \underline{x}_i^\intercal \underline{x}_j \hspace{1cm} \leftarrow \text{ scalar product between 2 samples}
\]
Consider:
\[
\underline{x}_i = 
\begin{bmatrix}
    x_{i1}\\
    x_{i2}
\end{bmatrix}
\hspace{0.5cm} \rightarrow \hspace{0.5cm}
\phi(\underline{x}_i) =
\begin{bmatrix}
    x_{i1}^2\\
    \sqrt{2} x_{i1}x_{i2}\\
    x_{i2}^2\\
\end{bmatrix}    
\]
\[
    \phi(\underline{x}_i)^\intercal \phi(\underline{x}_j) = x_{i1}^2x_{j1}^2 + 2x_{i1}x_{i2}x_{j1}x_{j2} + x_{i2}^2x_{j2}^2 = (\star)
\]
In order to compute ($\star$) we have to apply the feature map twice and the scalar product between the two vectors. We can avoid this by using the kernel function:
\[
    K(\underline{x}_i, \underline{x}_j) = (\underline{x}_i^\intercal \underline{x}_j)^2 = (x_{i1}x_{j1} + x_{i2}x_{j2})^2 = (\star)    
\]
But here i'm computing this product immediately, without applying the feature map.WE have computed the scalar product in a cheaper way.

Typical kernel functions family:
\begin{enumerate}[i]
    \item Polynomials of degree $q$: $K(\underline{x}_i, \underline{x}_j) = (\underline{x}_i^\intercal \underline{x}_j)^q$
    \item Polynomials of degree less or equal to $q$: $K(\underline{x}_i, \underline{x}_j) = (\underline{x}_i^\intercal \underline{x}_j + 1)^q$
    \item Gaussian RBF: $K(\underline{x}_i, \underline{x}_j) = \exp(-\frac{||\underline{x}_i - \underline{x}_j||_2^2}{2\sigma^2})$
\end{enumerate}
Consider again the value of alpha:
\begin{multicols}{2}
    \[
        \begin{split}
            \underline{\alpha} &= (\underbrace{\Phi\Phi^\intercal}_{K \in \mathbb{R}^{n\times n}} + \lambda I)^{-1} \underline{y} \\
            &= (K + \lambda I)^{-1} \underline{y}\\    
        \end{split}
\]
\vspace{0.2cm}
\[
    \begin{split}
        &\Phi \in \mathbb{R}^{n \times d}    \\
    &K_{ij} = \phi(\underline{x}_i)^\intercal \phi(\underline{x}_j) = K(\underline{x}_i, \underline{x}_j)   
    \end{split} 
\]
\end{multicols}
How to predict a label from a new sample?
\[
    \begin{split}
        \hat{y} &= \phi(\underline{x})^\intercal \underline{\hat{w}}_R \\
        &= \underline{\hat{w}}_R^\intercal \phi(\underline{x})\\
        &= (\Phi^\intercal \underline{\alpha})\phi(\underline{x})\\
        &= \underline{\alpha}^\intercal \Phi \phi(\underline{x})\\
        &= \sum_{i=1}^n \alpha_i \phi(\underline{x}_i)^\intercal \phi(\underline{x})\\
        &= \sum_{i=1}^n \alpha_i K(\underline{x}_i, \underline{x})\\
    \end{split}
\]
We are doing a scalar product between the already present samples in the dataset and the new one. All weighted by the value of $\alpha$ which represent how much each sample contribute to the definition of the decision boundary. Recall that the scalar product is a similarity measure so there will be samples more or less similar to the one to predict. 