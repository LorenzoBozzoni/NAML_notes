In the last lecture we have introduced the least squares method. In particular we have mentioned the linear model for which:
\[
    \hat{\underline{y}} = X\hat{\underline{w}} \hspace{1cm} X \in \mathbb{R}^{n \times p} \hspace{0.4cm} \underline{\hat{y}} \in \mathbb{R}^n \hspace{0.4cm} n \geq p \hspace{0.4cm} X \text{ full rank}
\]
We have obtained:
\[
    \underline{\hat{w}} = (X^\intercal X)^{-1} X^\intercal \underline{y} \implies \underline{\hat{y}} = \underbrace{X(X^\intercal X)^{-1} X^\intercal}_{P_x} \underline{y}     
\]
The matrix $P_x$ dimension is given by the product of: $(n\times p)(p\times p)(p \times n) = (n\times n)$ and has this properties:
\begin{itemize}
    \item $P_x = P_x^2$
    \item $P_x$ is a projection matrix
\end{itemize}

Let's consider $U$ an orthogonal ($U^\intercal U = I$) matrix that contains the basis for $\mathcal{C}(X)$ this means that $\mathcal{C}(X) = \mathcal{C}(U)$. We can write:
\[
    \underline{\hat{y}} = X\underline{\hat{w}} = U\underline{\tilde{w}}    
\]
So this basically means that the predicted value of $y$ still a projection on a plane but this time the plane is spanned by the columns of $U$ and not by the columns of $X$. By substituting last equation in the minimization method for least squares we have:
\[
    \underline{\tilde{w}} = \underset{\underline{w}}{\arg\min} ||\underline{y} - U\underline{w}||^2_2 \implies \underline{\hat{y}} = U\underline{\tilde{w}} = U(U^\intercal U)^{-1}U^\intercal \underline{y} = UU^\intercal \underline{y} 
\]
This formulation is possible because this time in the parenthesis we have an orthogonal matrix and this means that $(U^\intercal U)^{-1} = U^\intercal U = I$. In general $UU^\intercal \neq I$ because it might be rectangular (while $U^\intercal U$ is always square).\\

\textbf{Example of usage of U}
We start from $X$:
\[
    X= \begin{bmatrix}
        1& 1 \\
        1 & 3\\
        0 & 0
    \end{bmatrix}
    \hspace{1cm}
    \underline{x_1} = \begin{bmatrix}
        1\\
        1\\
        0   
    \end{bmatrix}
    \hspace{1cm}
    \underline{x_2} = \begin{bmatrix}
        1\\
        3\\
        0
    \end{bmatrix}
\]
How do we build the orthogonal matrix $U$? We can use the Gram-Schmidt procedure:
\[
    \underline{u_1} = \dfrac{\underline{x_1}}{||\underline{x_1}||} = \begin{bmatrix}
        \dfrac{1}{\sqrt{2}}\\
        \dfrac{1}{\sqrt{2}}\\
        0
    \end{bmatrix}    
\]
\[
    \underline{{x'}_2} = \underline{x_2} - (\underline{x_2}^\intercal \underline{u_1})\underline{u_1} = \underline{x_2} - (\underline{u_1^\intercal} \underline{u_1})\underline{x_2} \implies 
    \underline{u_2} = \dfrac{\underline{{x'}_2}}{||\underline{{x'}_2}||} 
    = \begin{bmatrix}
        -\dfrac{1}{\sqrt{2}}\\
        \dfrac{1}{\sqrt{2}}\\
        0
    \end{bmatrix}    
\]
So, the overall matrix $U$ is:
\[
    U = \begin{bmatrix}
        \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}\\
        -\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}}\\
        0 & 0
    \end{bmatrix}    \hspace{1cm}
    U^\intercal U = I \text{ and } UU^\intercal \neq I
\]
A drawback of Gram Schmidt is that, depending on the order chosen for the columns of $X$, the matrix $U$ can be different. Moreover, the order of vector columns of $U$ is meaningless. \\

Now, we want to exploit the SVD for computing the orthogonal matrix $U$. We start from the SVD of $X$:
\[
    X = U\Sigma V^\intercal    
\]
So
\begin{multicols}{2}
    \[
    \begin{split}
        \underline{\hat{w}} &= (X^\intercal X)^{-1} X^\intercal y\\
        & = (V\Sigma^\intercal U^\intercal U\Sigma V^\intercal)^{-1} V\Sigma^\intercal U^\intercal \underline{y} \\
        & = (V\Sigma^\intercal \Sigma V^\intercal)^{-1} V\Sigma^\intercal U^\intercal \underline{y}\\
        & = V(V\Sigma^\intercal \Sigma V^\intercal)^{-1} V\Sigma^\intercal U^\intercal \underline{y}\\
        & = V(\Sigma^\intercal \Sigma)^{-1} \underbrace{V^\intercal V}_{I}\Sigma^\intercal U^\intercal \underline{y}\\
        & = V\underbrace{(\Sigma^\intercal \Sigma)^{-1} \Sigma^\intercal}_{\Sigma^+} U^\intercal \underline{y}\\ 
        & = V\Sigma^+ U^\intercal \underline{y}
    \end{split}
\]
Recall that:
\[
(AB)^{-1} = B^{-1} A^{-1}    
\]
\[
(V^\intercal)^{-1} = V     
\]
Because $V$ is orthogonal.\\
$\Sigma^+$ il called the pseudo-inverse of $\Sigma$.
\end{multicols}
Eventually, we have:
\[
    \begin{split}
        \underline{\hat{y}} &= X(X^\intercal X)^{-1} X^\intercal \underline{y} = XX^+ \underline{y}\\ 
        &= U(U^\intercal U)^{-1} U^\intercal \underline{y} = UU^+ \underline{y}
    \end{split}
\]
Recalling that we are considering the case in which $n \geq p$, the matrices have these dimensions: $U = (n \times n), \Sigma = (n \times p), V^\intercal = (p \times p)$ and:
\[
    \Sigma = \begin{bmatrix}
        \sigma_1 & 0 & \dots & 0\\
        0 & \sigma_2 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & \sigma_p\\
        0 & 0 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & 0
    \end{bmatrix}
\]
\[
    \Sigma^\intercal \Sigma = \begin{bmatrix}
        \sigma_1 & 0 & \dots & 0 & 0 & \dots & 0\\
        0 & \sigma_2 & \dots & 0 & 0 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & \sigma_p & 0 & \dots & 0
    \end{bmatrix} 
    \begin{bmatrix}
        \sigma_1 & 0 & \dots & 0\\
        0 & \sigma_2 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & \sigma_p\\
        0 & 0 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & 0
    \end{bmatrix}
    =
    \begin{bmatrix}
        \sigma_1^2 & 0 & \dots & 0\\
        0 & \sigma_2^2 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & \sigma_p^2\\
    \end{bmatrix}
\]
\[
    (\Sigma^\intercal \Sigma)^{-1} = \begin{bmatrix}
        \dfrac{1}{\sigma_1^2} & 0 & \dots & 0\\
        0 & \dfrac{1}{\sigma_2^2} & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & \dfrac{1}{\sigma_p^2}\\
    \end{bmatrix}    
\]
\[
    \Sigma^+ =  (\Sigma^\intercal \Sigma)^{-1} \Sigma^\intercal = \begin{bmatrix}
        \dfrac{1}{\sigma_1} & 0 & \dots & 0 & 0 & \dots & 0\\
        0 & \dfrac{1}{\sigma_2} & \dots & 0 & 0 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & \dfrac{1}{\sigma_p} & 0 & \dots & 0
    \end{bmatrix}
\]

Let's consider now the case in which $p \geq n$ and $X$ has $n$ linearly independent rows (before we had $p$ linearly independent columns). This means that we have more unknowns than equations and we would find infinite solutions for $\underline{\hat{w}}$ such that $\underline{\hat{y}} = X\underline{\hat{w}}$. 

The solution found before $\underline{\hat{w}} = V\Sigma^+ U^\intercal \underline{y}$ it's still valid but now $\Sigma^+ = \Sigma^\intercal(\Sigma\Sigma^\intercal)^{-1}$ so it has the same shape as before but transposed. 
Summary:
\begin{multicols}{2}
    \begin{center}
        $n > p$\\
        \vspace{0.3cm}
        \begin{tikzpicture}
            \draw (-1,0) -- (1,0) -- (1,4) -- (-1,4) -- (-1,0) -- (0,0) node[below]{$n$};
            \draw (1,0) -- (1,2) node[right]{$p$};
        \end{tikzpicture}
        \[
            \underline{\hat{w}} = V\Sigma^+ U^\intercal \underline{y}    
        \]
        \[
            \Sigma^+ = (\Sigma^\intercal \Sigma)^{-1} \Sigma^\intercal
        \]
    \end{center}
    \newcolumn
    \begin{center}
        $p > n$\\
        \vspace{0.3cm}
        \begin{tikzpicture}
            \draw (-2,0) -- (2,0) -- (2,2) -- (-2,2) -- (-2,0) -- (0,0) node[below]{$p$};
            \draw (2,0) -- (2,1) node[right]{$n$};
        \end{tikzpicture}
        \[
            \underline{\hat{w}} = V\Sigma^+ U^\intercal \underline{y}    
        \]
        \[
            \Sigma^+ = \Sigma^\intercal(\Sigma\Sigma^\intercal)^{-1}    
        \]

    \end{center}
\end{multicols}

Let's consider two vectors: $\underline{w}$ and $\underline{\hat{w}}$:
\[
    \underline{y} = x\underline{w} = X\underline{\hat{w}}     
\]
So they both are solutions of the system of equations.
\begin{multicols}{2}
\[
    \begin{split}
        ||\underline{w}||_2^2 &= ||(\underline{w} - \hat{\underline{w}}) + \hat{\underline{w}}||_2^2\\
        &= ||\underline{w} - \hat{\underline{w}}||^2_2 - 2\underbrace{(\underline{w}-\hat{\underline{w}})^\intercal \hat{\underline{w}}}_{\star} + ||\hat{\underline{w}}||^2_2    \\
        &= \underbrace{||\underline{w} - \hat{\underline{w}}||_2^2}_{\geq 0} + ||\hat{\underline{w}}||^2_2 
    \end{split}
\]    
\[
    \begin{split}
        \star &= (\underline{w} - \hat{\underline{w}})^\intercal X^\intercal(XX^\intercal) \underline{y}\\
        &= (X(\underline{w} - \hat{\underline{w}}))^\intercal (XX^\intercal)^{-1} \underline{y}\\
        &= (X\underline{w} - X\hat{\underline{w}})^\intercal (XX^\intercal)^{-1} \underline{y}\\
        &= (0)^\intercal (XX^\intercal)^{-1} \underline{y}\\
        &= 0
    \end{split}    
\]
\end{multicols}
So
\[
    \implies ||\underline{w}||_2^2 \geq ||\hat{\underline{w}}||^2_2     
\]
This means that $\underline{\hat{w}}$ has minimum norm solution, i.e. among all possible vectors $w$ that satisfy the initial equation $ \underline{y} = x\underline{w} = X\underline{\hat{w}}$, $\underline{\hat{w}}$ is the one with the minimum norm. In order to understand why that property is important, consider the following example.\\

\noindent Suppose to have:
\[
    X = 
\begin{bmatrix}
    1 & 0 & 0,01\\
    0 & 1 & 0
\end{bmatrix}
\hspace{1cm}
\underline{y} = 
\begin{bmatrix}
    1\\
    0\\
\end{bmatrix}    
\]
We could choose:
\[
    \underline{w_1} = \begin{bmatrix}
        1\\
        0\\
        0\\
    \end{bmatrix}
    \hspace{1cm}
    \underline{w_2} = \begin{bmatrix}
        0\\
        0\\
        100\\
    \end{bmatrix}
\]
It is easily understandable that, if would choose the second vector as a solution instead of the first one, we would have an important amplification of the error related to the measure of the value of the column feature. The same cannot be said for the first vector.

Notice that, it is not that improbable to have more features than samples, consider for example images, for which each pixel is a feature.

\section{Matrix completion}
We start from 
\[
    X \in \mathbb{R}^{n \times p}  \hspace{0.5cm} rank(X) = r \ll \min(n,p) 
\]
We know only partially $X$, we know $X_{i,j}$ \texttt{for} $(i,j) \in \Omega$. 

Matrix completion is a method for filling missing values. If we had full rank matrix we would have very independent columns so we would not be able to retrieve/obtain missing values. Being low rank, in this sense, helps us accomplishing this goal.  

\subsection{Ideal estimator}
\[
    \begin{cases}
        \hat{X} = \underset{z \in \mathbb{R}^{n \times p}}{\arg \min} \left[\text{ rank}(z)\right]\\
        \text{subject to } \hat{X_{ij}} = X_{ij} (i,j) \in \Omega
    \end{cases}    
\]
This formulation is computationally unfeasible and the object function is non-convex. What is in practical terms saying is that, amongs all possible solutions, we want the one with minimal rank.

How can we solve it?

\subsection{Practical estimator}
We are going to use the \textbf{Nuclear norm}: $||\cdot||_*  = \sum\limits_{i=1}^{\min(n,p)}\sigma_i$.

The idea is that, instead of minimizing the rank, we minimize the norm since, when performing the SVD, the number of non-zero singular values is equal to the rank of the matrix.
\[
    \begin{cases}
        \hat{X} = \underset{z \in \mathbb{R}^{n \times p}}{\arg \min} ||z||_*\\
        \text{subject to } \hat{X_{ij}} = X_{ij} (i,j) \in \Omega
    \end{cases}    
\]
This new formulation is convex-optimal. How are we going to solve this problem? With the \textbf{SVT (Singular Value Threshold)}. Here is the algorithm:
\begin{itemize}
    \item Initialize $\hat{X} = \texttt{zeros}(n,p)$
    \item Set $\hat{X}_{ij} = X_{ij} \texttt{ for } (i,j) \in \Omega$
    \item For $k = 1,2, \dots, N$  
    \begin{itemize}
        \item $\hat{X}_{old} = \hat{X}$
        \item $[U, \Sigma, V^\intercal] = \texttt{svd}(\hat{X})$
        \item $\Sigma \rightarrow \hat{\Sigma} \hspace{0.2cm} \begin{cases}
            \hat{\sigma}_i = \sigma_i & \text{ if } \sigma_i > \tau\\
            \hat{\sigma}_i = 0 & \text{ otherwise }
        \end{cases}$
        \item $\hat{X} = U\hat{\Sigma}V^\intercal$
        \item $\hat{X}_{ij} = X_{ij} \texttt{ for } (i,j) \in \Omega$
    \end{itemize}
    \item $||\hat{X} - X_{old}|| < \epsilon$
\end{itemize}
The constant $\tau$ is imposing some sort of thresholding on the acceptance of singular values. This is an example of non-monothone algorithm because after the reduction of rank with the condition on sigmas, we override some value of the matrix to the one contained in $\Omega$. Is it proved that for a large enough index $k$ the algorithm converge to the solution. 